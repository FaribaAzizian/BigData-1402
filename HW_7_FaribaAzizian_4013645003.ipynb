{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPW9GCMnbv5QbVM8ZDDWnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaribaAzizian/BigData-1402/blob/main/HW_7_FaribaAzizian_4013645003.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "lfafDfIMp-89"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.3.2-bin-hadoop3'"
      ],
      "metadata": {
        "id": "BH1amijwqmhY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "d99tl234qwEH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CqlX-VuRrZi-",
        "outputId": "b63cbaa2-4a45-43b5-b8fb-1b83d1a716e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.3.2-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark --quiet\n",
        "!pip install -q handyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqcO3BTNraH7",
        "outputId": "fc2657f7-554a-4217-c324-4524e3aeae91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRjOprk566Tt",
        "outputId": "97ab6045-a5f8-4dcc-a58f-3982d1f0f4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Uber_dataset.\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "uber_data = spark.read.options(inferSchema='True',header='True',sep=',').csv(\"/content/drive/MyDrive/uber-raw-data-aug14.csv\")\n",
        "\n",
        "for col in uber_data.columns:\n",
        "  print(col, \"\\t\", \"with null values: \", uber_data.filter(uber_data[col].isNull()).count())\n",
        "uber_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJN8yaooDEt",
        "outputId": "93200ab9-af1b-4dcd-9fdb-13fdd0a8d11a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date/Time \t with null values:  0\n",
            "Lat \t with null values:  0\n",
            "Lon \t with null values:  0\n",
            "Base \t with null values:  0\n",
            "+----------------+-------+--------+------+\n",
            "|       Date/Time|    Lat|     Lon|  Base|\n",
            "+----------------+-------+--------+------+\n",
            "|8/1/2014 0:03:00|40.7366|-73.9906|B02512|\n",
            "|8/1/2014 0:09:00| 40.726|-73.9918|B02512|\n",
            "|8/1/2014 0:12:00|40.7209|-74.0507|B02512|\n",
            "|8/1/2014 0:12:00|40.7387|-73.9856|B02512|\n",
            "|8/1/2014 0:12:00|40.7323|-74.0077|B02512|\n",
            "|8/1/2014 0:13:00|40.7349|-74.0033|B02512|\n",
            "|8/1/2014 0:15:00|40.7279|-73.9542|B02512|\n",
            "|8/1/2014 0:17:00| 40.721|-73.9937|B02512|\n",
            "|8/1/2014 0:19:00|40.7195| -74.006|B02512|\n",
            "|8/1/2014 0:20:00|40.7448|-73.9799|B02512|\n",
            "|8/1/2014 0:21:00|40.7399|-74.0057|B02512|\n",
            "|8/1/2014 0:25:00|40.7651|-73.9683|B02512|\n",
            "|8/1/2014 0:27:00|40.7354|-74.0081|B02512|\n",
            "|8/1/2014 0:29:00|40.7339|-74.0028|B02512|\n",
            "|8/1/2014 0:29:00|40.7364|-74.0301|B02512|\n",
            "|8/1/2014 0:29:00|40.7364|-74.0301|B02512|\n",
            "|8/1/2014 0:30:00|40.7252|-73.9516|B02512|\n",
            "|8/1/2014 0:30:00|40.7433| -73.986|B02512|\n",
            "|8/1/2014 0:34:00|40.7437|-73.9884|B02512|\n",
            "|8/1/2014 0:36:00|40.7406|-74.0077|B02512|\n",
            "+----------------+-------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uber_data.printSchema()"
      ],
      "metadata": {
        "id": "0vfm37bL7ZFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed092e6a-c791-46db-90f4-5ef951bedf25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date/Time: string (nullable = true)\n",
            " |-- Lat: double (nullable = true)\n",
            " |-- Lon: double (nullable = true)\n",
            " |-- Base: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from handyspark import *\n",
        "uber_plot = uber_data.toHandy()\n",
        "uber_plot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8lW0zoyywgM",
        "outputId": "8f5366b4-135b-4f98-c365-31f39dce7fc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------+--------+------+\n",
            "|       Date/Time|    Lat|     Lon|  Base|\n",
            "+----------------+-------+--------+------+\n",
            "|8/1/2014 0:03:00|40.7366|-73.9906|B02512|\n",
            "|8/1/2014 0:09:00| 40.726|-73.9918|B02512|\n",
            "|8/1/2014 0:12:00|40.7209|-74.0507|B02512|\n",
            "|8/1/2014 0:12:00|40.7387|-73.9856|B02512|\n",
            "|8/1/2014 0:12:00|40.7323|-74.0077|B02512|\n",
            "|8/1/2014 0:13:00|40.7349|-74.0033|B02512|\n",
            "|8/1/2014 0:15:00|40.7279|-73.9542|B02512|\n",
            "|8/1/2014 0:17:00| 40.721|-73.9937|B02512|\n",
            "|8/1/2014 0:19:00|40.7195| -74.006|B02512|\n",
            "|8/1/2014 0:20:00|40.7448|-73.9799|B02512|\n",
            "|8/1/2014 0:21:00|40.7399|-74.0057|B02512|\n",
            "|8/1/2014 0:25:00|40.7651|-73.9683|B02512|\n",
            "|8/1/2014 0:27:00|40.7354|-74.0081|B02512|\n",
            "|8/1/2014 0:29:00|40.7339|-74.0028|B02512|\n",
            "|8/1/2014 0:29:00|40.7364|-74.0301|B02512|\n",
            "|8/1/2014 0:29:00|40.7364|-74.0301|B02512|\n",
            "|8/1/2014 0:30:00|40.7252|-73.9516|B02512|\n",
            "|8/1/2014 0:30:00|40.7433| -73.986|B02512|\n",
            "|8/1/2014 0:34:00|40.7437|-73.9884|B02512|\n",
            "|8/1/2014 0:36:00|40.7406|-74.0077|B02512|\n",
            "+----------------+-------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
        "uber_plot.cols['Date/Time'].hist(ax=axs[0])\n",
        "uber_plot.cols['Lat'].hist(ax=axs[1])\n",
        "uber_plot.cols['Lon'].hist(ax=axs[2])\n",
        "uber_plot.cols['Base'].hist(ax=axs[2])"
      ],
      "metadata": {
        "id": "G5GPpz2AzBOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "all_uber_data=['Date/Time','Lat','Lon','Base']\n",
        "for col in all_uber_data:\n",
        "  outCol='i_'+col\n",
        "  uber_plot=uber_plot.drop(outCol)\n",
        "  uber_plot = StringIndexer(inputCol=col,outputCol=outCol).fit(uber_plot).transform(uber_plot)\n",
        "\n",
        "uber_plot=uber_plot.drop('Date/Time','Lat','Lon','Base')\n",
        "\n",
        "uber_plot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsicsZAmzjNf",
        "outputId": "098e6030-104e-468d-9b71-c05f6178ebfd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+------+------+\n",
            "|i_Date/Time|i_Lat| i_Lon|i_Base|\n",
            "+-----------+-----+------+------+\n",
            "|    20173.0|431.0| 188.0|   4.0|\n",
            "|    34403.0|111.0|   3.0|   4.0|\n",
            "|    25098.0|115.0|3094.0|   4.0|\n",
            "|    25098.0| 88.0|  55.0|   4.0|\n",
            "|    25098.0|364.0|  66.0|   4.0|\n",
            "|    29513.0|390.0| 193.0|   4.0|\n",
            "|    20174.0|270.0| 575.0|   4.0|\n",
            "|    29515.0| 99.0|  47.0|   4.0|\n",
            "|    26643.0|170.0| 170.0|   4.0|\n",
            "|    29516.0| 39.0| 164.0|   4.0|\n",
            "|    32031.0| 80.0|  60.0|   4.0|\n",
            "|    29518.0|394.0| 536.0|   4.0|\n",
            "|    30819.0|538.0| 219.0|   4.0|\n",
            "|    20175.0|349.0| 349.0|   4.0|\n",
            "|    20175.0|442.0|1347.0|   4.0|\n",
            "|    20175.0|442.0|1347.0|   4.0|\n",
            "|    33228.0|308.0| 571.0|   4.0|\n",
            "|    33228.0|318.0| 174.0|   4.0|\n",
            "|    26644.0|163.0|  84.0|   4.0|\n",
            "|    32034.0|  6.0|  66.0|   4.0|\n",
            "+-----------+-----+------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "vecAssembler = VectorAssembler(inputCols=[\"i_Date/Time\",\"i_Lat\", \"i_Lon\",\"i_Base\"], outputCol=\"features\")\n",
        "new_uber = vecAssembler.transform(uber_plot)\n",
        "new_uber.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anug5J7jmQjt",
        "outputId": "66f7c978-a28c-4ed2-fc3c-c03376299c66"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+------+------+--------------------+\n",
            "|i_Date/Time|i_Lat| i_Lon|i_Base|            features|\n",
            "+-----------+-----+------+------+--------------------+\n",
            "|    20173.0|431.0| 188.0|   4.0|[20173.0,431.0,18...|\n",
            "|    34403.0|111.0|   3.0|   4.0|[34403.0,111.0,3....|\n",
            "|    25098.0|115.0|3094.0|   4.0|[25098.0,115.0,30...|\n",
            "|    25098.0| 88.0|  55.0|   4.0|[25098.0,88.0,55....|\n",
            "|    25098.0|364.0|  66.0|   4.0|[25098.0,364.0,66...|\n",
            "|    29513.0|390.0| 193.0|   4.0|[29513.0,390.0,19...|\n",
            "|    20174.0|270.0| 575.0|   4.0|[20174.0,270.0,57...|\n",
            "|    29515.0| 99.0|  47.0|   4.0|[29515.0,99.0,47....|\n",
            "|    26643.0|170.0| 170.0|   4.0|[26643.0,170.0,17...|\n",
            "|    29516.0| 39.0| 164.0|   4.0|[29516.0,39.0,164...|\n",
            "|    32031.0| 80.0|  60.0|   4.0|[32031.0,80.0,60....|\n",
            "|    29518.0|394.0| 536.0|   4.0|[29518.0,394.0,53...|\n",
            "|    30819.0|538.0| 219.0|   4.0|[30819.0,538.0,21...|\n",
            "|    20175.0|349.0| 349.0|   4.0|[20175.0,349.0,34...|\n",
            "|    20175.0|442.0|1347.0|   4.0|[20175.0,442.0,13...|\n",
            "|    20175.0|442.0|1347.0|   4.0|[20175.0,442.0,13...|\n",
            "|    33228.0|308.0| 571.0|   4.0|[33228.0,308.0,57...|\n",
            "|    33228.0|318.0| 174.0|   4.0|[33228.0,318.0,17...|\n",
            "|    26644.0|163.0|  84.0|   4.0|[26644.0,163.0,84...|\n",
            "|    32034.0|  6.0|  66.0|   4.0|[32034.0,6.0,66.0...|\n",
            "+-----------+-----+------+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "rows = []\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(k=k, seed=1, maxIter=10)\n",
        "    model = kmeans.fit(new_uber.select('features'))\n",
        "    transformed = model.transform(new_uber)\n",
        "    trainingCost = model.summary.trainingCost\n",
        "    clusterSizes = model.summary.clusterSizes\n",
        "    silhouette_scores = evaluator.evaluate(transformed)\n",
        "    row = [k,trainingCost,str(clusterSizes),silhouette_scores]\n",
        "    rows += [row]\n",
        "\n",
        "result = [\"K\", \"trainingCost\", \"clusterSizes\",\"silhouette\"]\n",
        "cluster = spark.createDataFrame(rows, result)\n",
        "\n",
        "cluster.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWR0rgxOpBgw",
        "outputId": "0e4157ef-eb3c-4388-f2e6-c0c890a646a2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+------------------+\n",
            "|  K|        trainingCost|        clusterSizes|        silhouette|\n",
            "+---+--------------------+--------------------+------------------+\n",
            "|  2|2.785176303328717E13|    [322511, 506764]|0.7850548176181351|\n",
            "|  3|1.313058617415897...|[279017, 361231, ...|0.7413247729585755|\n",
            "|  4|7.828743161814183E12|[195901, 271594, ...|0.7146745050180923|\n",
            "|  5|5.625367056407512E12|[213634, 279520, ...|0.7073278747177604|\n",
            "|  6|3.984104825519183...|[152901, 231049, ...|0.6938777628385723|\n",
            "|  7|3.106021216592139...|[142099, 204319, ...|0.6819539518405358|\n",
            "|  8|2.596557139041030...|[133306, 68725, 1...| 0.671663266855305|\n",
            "|  9|2.034790075362940...|[99255, 158921, 3...|0.6553532809735969|\n",
            "| 10|1.737081981313130...|[84283, 130147, 3...|0.6376442589033418|\n",
            "+---+--------------------+--------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = new_uber.randomSplit([0.8, 0.2], seed=1)\n",
        "print('Count of Training Dataset : ' , train.count())\n",
        "print('Count of Testing Dataset: ' , test.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g0Lg4mUgTRo",
        "outputId": "0d0a7222-221d-418d-8c3a-d0a3c4b87fe6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Training Dataset :  663303\n",
            "Count of Testing Dataset:  165972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we choose k = 3\n",
        "k_means = KMeans(k = 3, seed = 1, maxIter = 10, featuresCol = \"features\", predictionCol = \"prediction\")\n",
        "\n",
        "k_model = kmeans.fit(train)\n",
        "k_transformed = k_model.transform(test)\n",
        "\n",
        "k_transformed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeJz9_qljAu4",
        "outputId": "1538b3f7-d31f-4d3c-859e-c5e116bcb5a4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+------+------+--------------------+----------+\n",
            "|i_Date/Time|i_Lat| i_Lon|i_Base|            features|prediction|\n",
            "+-----------+-----+------+------+--------------------+----------+\n",
            "|        0.0| 23.0| 198.0|   0.0|[0.0,23.0,198.0,0.0]|         6|\n",
            "|        0.0| 95.0| 445.0|   0.0|[0.0,95.0,445.0,0.0]|         6|\n",
            "|        0.0| 97.0|  19.0|   4.0| [0.0,97.0,19.0,4.0]|         6|\n",
            "|        0.0|101.0| 254.0|   0.0|[0.0,101.0,254.0,...|         6|\n",
            "|        0.0|111.0| 104.0|   4.0|[0.0,111.0,104.0,...|         6|\n",
            "|        0.0|211.0| 318.0|   1.0|[0.0,211.0,318.0,...|         6|\n",
            "|        0.0|256.0|1767.0|   0.0|[0.0,256.0,1767.0...|         6|\n",
            "|        0.0|264.0| 564.0|   0.0|[0.0,264.0,564.0,...|         6|\n",
            "|        0.0|377.0|1442.0|   0.0|[0.0,377.0,1442.0...|         6|\n",
            "|        0.0|398.0| 200.0|   0.0|[0.0,398.0,200.0,...|         6|\n",
            "|        0.0|513.0| 372.0|   1.0|[0.0,513.0,372.0,...|         6|\n",
            "|        0.0|674.0| 139.0|   1.0|[0.0,674.0,139.0,...|         6|\n",
            "|        1.0|  5.0| 285.0|   0.0| [1.0,5.0,285.0,0.0]|         6|\n",
            "|        1.0| 37.0| 141.0|   0.0|[1.0,37.0,141.0,0.0]|         6|\n",
            "|        1.0|112.0| 202.0|   1.0|[1.0,112.0,202.0,...|         6|\n",
            "|        1.0|120.0| 368.0|   0.0|[1.0,120.0,368.0,...|         6|\n",
            "|        1.0|192.0| 183.0|   1.0|[1.0,192.0,183.0,...|         6|\n",
            "|        1.0|204.0| 144.0|   1.0|[1.0,204.0,144.0,...|         6|\n",
            "|        1.0|242.0| 576.0|   0.0|[1.0,242.0,576.0,...|         6|\n",
            "|        1.0|308.0| 326.0|   0.0|[1.0,308.0,326.0,...|         6|\n",
            "+-----------+-----+------+------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTSJxQ6GtpFs",
        "outputId": "f5a0cc4c-7f6d-43bd-af3a-4710d6ae33cf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sSOL https://downloads.apache.org/kafka/2.7.0/kafka_2.13-2.7.0.tgz\n",
        "!tar -xzf kafka_2.13-2.7.0.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g6eHfiyuXx9",
        "outputId": "65ed4878-eb5c-42ba-e4e0-32e3c6460401"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: not in gzip format\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-2.7.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-2.7.0/config/zookeeper.properties\n",
        "!./kafka_2.13-2.7.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-2.7.0/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfIe-RsDu2YY",
        "outputId": "2bd97cfe-8fbb-4753-ccb6-fbfc2da367fe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./kafka_2.13-2.7.0/bin/zookeeper-server-start.sh: No such file or directory\n",
            "/bin/bash: ./kafka_2.13-2.7.0/bin/kafka-server-start.sh: No such file or directory\n",
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep kafka"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ll2TKkvFn1",
        "outputId": "cc85bdea-d142-4ff8-ad45-467a2e2a09a9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       84430     179  0 17:08 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n",
            "root       84432   84430  0 17:08 ?        00:00:00 grep kafka\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic reco-train\n",
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic reco-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QhMzzdWvMXB",
        "outputId": "e08fe8de-242f-4acb-821d-ed5509639d15"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./kafka_2.13-2.7.0/bin/kafka-topics.sh: No such file or directory\n",
            "/bin/bash: ./kafka_2.13-2.7.0/bin/kafka-topics.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic reco-train\n",
        "!./kafka_2.13-2.7.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic reco-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b3FkEI9vTbZ",
        "outputId": "d862d91f-80d1-47d8-ea1b-fbab27cabdea"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./kafka_2.13-2.7.0/bin/kafka-topics.sh: No such file or directory\n",
            "/bin/bash: ./kafka_2.13-2.7.0/bin/kafka-topics.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Store the train and test data in kafka\n",
        "\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "  write_to_kafka(\"reco-test\", zip(x_test, y_test))\n",
        "\n",
        "  #refrences: https://colab.research.google.com/github/recohut/notebook/blob/master/_notebooks/2021-06-25-kafka-spark-streaming-colab.ipynb#scrollTo=UP_Hyjy0uyPN"
      ],
      "metadata": {
        "id": "xF0qQ1d2vrYy"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaConsumer\n",
        "\n",
        "# To consume latest messages and auto-commit offsets\n",
        "consumer = KafkaConsumer('my-topic',\n",
        "                         group_id='my-group',\n",
        "                         bootstrap_servers=['localhost:9092'])\n",
        "for message in consumer:\n",
        "    # message value and key are raw bytes -- decode if necessary!\n",
        "    # e.g., for unicode: `message.value.decode('utf-8')`\n",
        "    print (\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition,\n",
        "                                          message.offset, message.key,\n",
        "                                          message.value))\n",
        "\n",
        "# consume earliest available messages, don't commit offsets\n",
        "KafkaConsumer(auto_offset_reset='earliest', enable_auto_commit=False)\n",
        "\n",
        "# consume json messages\n",
        "KafkaConsumer(value_deserializer=lambda m: json.loads(m.decode('ascii')))\n",
        "\n",
        "# consume msgpack\n",
        "KafkaConsumer(value_deserializer=msgpack.unpackb)\n",
        "\n",
        "# StopIteration if no message after 1sec\n",
        "KafkaConsumer(consumer_timeout_ms=1000)\n",
        "\n",
        "# Subscribe to a regex topic pattern\n",
        "consumer = KafkaConsumer()\n",
        "consumer.subscribe(pattern='^awesome.*')\n",
        "\n",
        "# Use multiple consumers in parallel w/ 0.9 kafka brokers\n",
        "# typically you would run each on a different server / process / CPU\n",
        "consumer1 = KafkaConsumer('my-topic',\n",
        "                          group_id='my-group',\n",
        "                          bootstrap_servers='my.server.com')\n",
        "consumer2 = KafkaConsumer('my-topic',\n",
        "                          group_id='my-group',\n",
        "                          bootstrap_servers='my.server.com')"
      ],
      "metadata": {
        "id": "Ha7b5NWXKeAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers=['broker1:1234'])\n",
        "\n",
        "# Asynchronous by default\n",
        "future = producer.send('my-topic', b'raw_bytes')\n",
        "\n",
        "# Block for 'synchronous' sends\n",
        "try:\n",
        "    record_metadata = future.get(timeout=10)\n",
        "except KafkaError:\n",
        "    # Decide what to do if produce request failed...\n",
        "    log.exception()\n",
        "    pass\n",
        "\n",
        "# Successful result returns assigned partition and offset\n",
        "print (record_metadata.topic)\n",
        "print (record_metadata.partition)\n",
        "print (record_metadata.offset)\n",
        "\n",
        "# produce keyed messages to enable hashed partitioning\n",
        "producer.send('my-topic', key=b'foo', value=b'bar')\n",
        "\n",
        "# encode objects via msgpack\n",
        "producer = KafkaProducer(value_serializer=msgpack.dumps)\n",
        "producer.send('msgpack-topic', {'key': 'value'})\n",
        "\n",
        "# produce json messages\n",
        "producer = KafkaProducer(value_serializer=lambda m: json.dumps(m).encode('ascii'))\n",
        "producer.send('json-topic', {'key': 'value'})\n",
        "\n",
        "# produce asynchronously\n",
        "for _ in range(100):\n",
        "    producer.send('my-topic', b'msg')\n",
        "\n",
        "def on_send_success(record_metadata):\n",
        "    print(record_metadata.topic)\n",
        "    print(record_metadata.partition)\n",
        "    print(record_metadata.offset)\n",
        "\n",
        "def on_send_error(excp):\n",
        "    log.error('I am an errback', exc_info=excp)\n",
        "    # handle exception\n",
        "\n",
        "# produce asynchronously with callbacks\n",
        "producer.send('my-topic', b'raw_bytes').add_callback(on_send_success).add_errback(on_send_error)\n",
        "\n",
        "# block until all async messages are sent\n",
        "producer.flush()\n",
        "\n",
        "# configure multiple retries\n",
        "producer = KafkaProducer(retries=5)"
      ],
      "metadata": {
        "id": "PrB9clWILGBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_kafka_online_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "  \n",
        "for mini_ds in k_transformed:\n",
        "  mini_ds = mini_ds.shuffle(buffer_size=32)\n",
        "  mini_ds = mini_ds.map(decode_kafka_online_item)\n",
        "  mini_ds = mini_ds.batch(32)\n",
        "  if len(mini_ds) > 0:\n",
        "    model.fit(mini_ds, epochs=3)"
      ],
      "metadata": {
        "id": "i_F0WAZtAl8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}